{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "**The dataset** is collected from UCI Machine Learning Repository through the following [link](https://archive.ics.uci.edu/ml/datasets/Unmanned+Aerial+Vehicle+%28UAV%29+Intrusion+Detection)\n",
    "\n",
    "This application is working in first dataset (Bidirectional-flow/Parrot Bebop1), combined first dataset can be [downloaded](http://mason.gmu.edu/~lzhao9/materials/data/UAV/data/pub_dataset1.mat) from Liang Zhao homepage.Bidirectional-flow mode will involve 9 features × 2 sources × 3 direction flow = 54 features for more info visit this [link](http://mason.gmu.edu/~lzhao9/materials/data/UAV/)\n",
    "\n",
    "extract data with its default name `pub_dataset1.mat` in `__data__` directory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['D', 'H', 'data_te', 'data_tr'])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# use HDF reader for matlab v7.3 files\n",
    "data = { k:np.array(v).T for k, v in h5py.File('./__data__/pub_dataset1.mat').items()}\n",
    "data.keys()"
   ]
  },
  {
   "source": [
    "$n$ is the number of training samples   \n",
    "$k$ is the number of feature   \n",
    "$n^{\\prime}$ is the number of testing samples    \n",
    "$k^{\\prime}$ is the number of feature computational components and k is the numbe of features.  \n",
    "The last column of `data_te` and `data_tr` is the label: `1 means UAV, 0 otherwise`\n",
    "\n",
    "--- \n",
    "$\\text{data_tr} \\in R^{n×(k+1)}$   \n",
    "$\\text{data_te} \\in R^{n^{\\prime}×(k+1)}$   \n",
    "$D \\in R^{k×1}$. The generation runtime for each feature.  \n",
    "$H \\in R^{k^{\\prime}×k}$. The incident matrix of the feature computational hypergraph (see the paper for details). \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seed(seed=1917):\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['data_tr'][:, :-1]\n",
    "y = data['data_tr'][:, -1]\n",
    "\n",
    "X_test = data['data_te'][:, :-1]\n",
    "y_test = data['data_te'][:, -1]"
   ]
  },
  {
   "source": [
    "## MLP\n",
    "### Accuracy 0.9937035566396278"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9937035566396278"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "reset_random_seed()\n",
    "model = MLPClassifier()\n",
    "model.fit(X, y)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(data, ae, encoding_layers_count=3):\n",
    "    data = np.asmatrix(data)\n",
    "\n",
    "    layer = data\n",
    "    for i in range(encoding_layers_count):\n",
    "        layer = layer*ae.coefs_[i] + ae.intercepts_[i]\n",
    "        encoder1 = np.tanh(layer)\n",
    "    \n",
    "    return np.asarray(layer)"
   ]
  },
  {
   "source": [
    "## Auto Encoder\n",
    "### Flipbit Accuracy 0.5536332179930796"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 87909.46438852\n",
      "Iteration 2, loss = 87907.26911447\n",
      "Iteration 3, loss = 87905.08867516\n",
      "Iteration 4, loss = 87902.91165426\n",
      "Iteration 5, loss = 87900.69464342\n",
      "Iteration 6, loss = 87898.45406284\n",
      "Iteration 7, loss = 87896.17534820\n",
      "Iteration 8, loss = 87893.85662279\n",
      "Iteration 9, loss = 87891.53259668\n",
      "Iteration 10, loss = 87889.19414346\n",
      "Iteration 11, loss = 87886.82789623\n",
      "Iteration 12, loss = 87884.44114813\n",
      "Iteration 13, loss = 87882.01880032\n",
      "Iteration 14, loss = 87879.56105016\n",
      "Iteration 15, loss = 87877.06515435\n",
      "Iteration 16, loss = 87874.55420496\n",
      "Iteration 17, loss = 87871.98707037\n",
      "Iteration 18, loss = 87869.36962808\n",
      "Iteration 19, loss = 87866.78570170\n",
      "Iteration 20, loss = 87864.17729422\n",
      "Iteration 21, loss = 87861.54654581\n",
      "Iteration 22, loss = 87858.85854800\n",
      "Iteration 23, loss = 87856.15758688\n",
      "Iteration 24, loss = 87853.42773949\n",
      "Iteration 25, loss = 87850.68005862\n",
      "Iteration 26, loss = 87847.90027719\n",
      "Iteration 27, loss = 87845.06829821\n",
      "Iteration 28, loss = 87842.18625978\n",
      "Iteration 29, loss = 87839.27858238\n",
      "Iteration 30, loss = 87836.30184048\n",
      "Iteration 31, loss = 87833.28457220\n",
      "Iteration 32, loss = 87830.21874650\n",
      "Iteration 33, loss = 87827.08285803\n",
      "Iteration 34, loss = 87823.91301647\n",
      "Iteration 35, loss = 87820.67934224\n",
      "Iteration 36, loss = 87817.36767887\n",
      "Iteration 37, loss = 87814.01787211\n",
      "Iteration 38, loss = 87810.58691512\n",
      "Iteration 39, loss = 87807.09199786\n",
      "Iteration 40, loss = 87803.51402740\n",
      "Iteration 41, loss = 87799.88163796\n",
      "Iteration 42, loss = 87796.17023974\n",
      "Iteration 43, loss = 87792.39835608\n",
      "Iteration 44, loss = 87788.54925115\n",
      "Iteration 45, loss = 87784.61096275\n",
      "Iteration 46, loss = 87780.62057345\n",
      "Iteration 47, loss = 87776.54203479\n",
      "Iteration 48, loss = 87772.39690760\n",
      "Iteration 49, loss = 87768.18195147\n",
      "Iteration 50, loss = 87763.88324543\n",
      "Iteration 51, loss = 87759.50177780\n",
      "Iteration 52, loss = 87755.07446528\n",
      "Iteration 53, loss = 87750.55488033\n",
      "Iteration 54, loss = 87745.97154618\n",
      "Iteration 55, loss = 87741.30904181\n",
      "Iteration 56, loss = 87736.57090107\n",
      "Iteration 57, loss = 87731.79315260\n",
      "Iteration 58, loss = 87726.91137854\n",
      "Iteration 59, loss = 87721.98315844\n",
      "Iteration 60, loss = 87716.99543409\n",
      "Iteration 61, loss = 87711.92492022\n",
      "Iteration 62, loss = 87706.78594960\n",
      "Iteration 63, loss = 87701.58761626\n",
      "Iteration 64, loss = 87696.36649147\n",
      "Iteration 65, loss = 87691.06447970\n",
      "Iteration 66, loss = 87685.68543952\n",
      "Iteration 67, loss = 87680.26156073\n",
      "Iteration 68, loss = 87674.82731098\n",
      "Iteration 69, loss = 87669.27222737\n",
      "Iteration 70, loss = 87663.71866664\n",
      "Iteration 71, loss = 87658.08758437\n",
      "Iteration 72, loss = 87652.40913516\n",
      "Iteration 73, loss = 87646.71327023\n",
      "Iteration 74, loss = 87640.92623912\n",
      "Iteration 75, loss = 87635.15432989\n",
      "Iteration 76, loss = 87629.30425626\n",
      "Iteration 77, loss = 87623.41265834\n",
      "Iteration 78, loss = 87617.46933237\n",
      "Iteration 79, loss = 87611.54021788\n",
      "Iteration 80, loss = 87605.54329429\n",
      "Iteration 81, loss = 87599.51297538\n",
      "Iteration 82, loss = 87593.47147934\n",
      "Iteration 83, loss = 87587.37238890\n",
      "Iteration 84, loss = 87581.27523275\n",
      "Iteration 85, loss = 87575.14872581\n",
      "Iteration 86, loss = 87568.97303072\n",
      "Iteration 87, loss = 87562.76655965\n",
      "Iteration 88, loss = 87556.58153933\n",
      "Iteration 89, loss = 87550.35264482\n",
      "Iteration 90, loss = 87544.09271773\n",
      "Iteration 91, loss = 87537.84047313\n",
      "Iteration 92, loss = 87531.55256002\n",
      "Iteration 93, loss = 87525.24604847\n",
      "Iteration 94, loss = 87518.96659691\n",
      "Iteration 95, loss = 87512.62800380\n",
      "Iteration 96, loss = 87506.30425879\n",
      "Iteration 97, loss = 87499.97052862\n",
      "Iteration 98, loss = 87493.60863060\n",
      "Iteration 99, loss = 87487.25989225\n",
      "Iteration 100, loss = 87480.88965980\n",
      "Iteration 101, loss = 87474.55798087\n",
      "Iteration 102, loss = 87468.16764912\n",
      "Iteration 103, loss = 87461.79653101\n",
      "Iteration 104, loss = 87455.43779920\n",
      "Iteration 105, loss = 87449.04195071\n",
      "Iteration 106, loss = 87442.67003263\n",
      "Iteration 107, loss = 87436.33391109\n",
      "Iteration 108, loss = 87429.90114793\n",
      "Iteration 109, loss = 87423.57525694\n",
      "Iteration 110, loss = 87417.23690047\n",
      "Iteration 111, loss = 87410.85049344\n",
      "Iteration 112, loss = 87404.49716504\n",
      "Iteration 113, loss = 87398.15288433\n",
      "Iteration 114, loss = 87391.82601555\n",
      "Iteration 115, loss = 87385.48433007\n",
      "Iteration 116, loss = 87379.18888040\n",
      "Iteration 117, loss = 87372.86110541\n",
      "Iteration 118, loss = 87366.55484729\n",
      "Iteration 119, loss = 87360.24732737\n",
      "Iteration 120, loss = 87353.96193657\n",
      "Iteration 121, loss = 87347.71726898\n",
      "Iteration 122, loss = 87341.42600177\n",
      "Iteration 123, loss = 87335.20940907\n",
      "Iteration 124, loss = 87328.92598429\n",
      "Iteration 125, loss = 87322.75067055\n",
      "Iteration 126, loss = 87316.52115287\n",
      "Iteration 127, loss = 87310.31947985\n",
      "Iteration 128, loss = 87304.16432116\n",
      "Iteration 129, loss = 87298.00327176\n",
      "Iteration 130, loss = 87291.80432809\n",
      "Iteration 131, loss = 87285.68143696\n",
      "Iteration 132, loss = 87279.56522272\n",
      "Iteration 133, loss = 87273.44990124\n",
      "Iteration 134, loss = 87267.35771654\n",
      "Iteration 135, loss = 87261.29091284\n",
      "Iteration 136, loss = 87255.20029281\n",
      "Iteration 137, loss = 87249.17619559\n",
      "Iteration 138, loss = 87243.15049751\n",
      "Iteration 139, loss = 87237.11579490\n",
      "Iteration 140, loss = 87231.13121324\n",
      "Iteration 141, loss = 87225.12850802\n",
      "Iteration 142, loss = 87219.17504680\n",
      "Iteration 143, loss = 87213.21558064\n",
      "Iteration 144, loss = 87207.29608281\n",
      "Iteration 145, loss = 87201.36611101\n",
      "Iteration 146, loss = 87195.47184479\n",
      "Iteration 147, loss = 87189.59363460\n",
      "Iteration 148, loss = 87183.74820884\n",
      "Iteration 149, loss = 87177.87753210\n",
      "Iteration 150, loss = 87172.03489663\n",
      "Iteration 151, loss = 87166.23188252\n",
      "Iteration 152, loss = 87160.39196837\n",
      "Iteration 153, loss = 87154.66159353\n",
      "Iteration 154, loss = 87148.86496844\n",
      "Iteration 155, loss = 87143.12065989\n",
      "Iteration 156, loss = 87137.38267428\n",
      "Iteration 157, loss = 87131.68211485\n",
      "Iteration 158, loss = 87125.96130304\n",
      "Iteration 159, loss = 87120.27814434\n",
      "Iteration 160, loss = 87114.61984523\n",
      "Iteration 161, loss = 87108.96966585\n",
      "Iteration 162, loss = 87103.30722911\n",
      "Iteration 163, loss = 87097.71212791\n",
      "Iteration 164, loss = 87092.11051417\n",
      "Iteration 165, loss = 87086.50615581\n",
      "Iteration 166, loss = 87080.95093956\n",
      "Iteration 167, loss = 87075.37181234\n",
      "Iteration 168, loss = 87069.84456509\n",
      "Iteration 169, loss = 87064.34130677\n",
      "Iteration 170, loss = 87058.79299823\n",
      "Iteration 171, loss = 87053.28967698\n",
      "Iteration 172, loss = 87047.83472187\n",
      "Iteration 173, loss = 87042.35450989\n",
      "Iteration 174, loss = 87036.90397756\n",
      "Iteration 175, loss = 87031.46307817\n",
      "Iteration 176, loss = 87026.04667328\n",
      "Iteration 177, loss = 87020.63709448\n",
      "Iteration 178, loss = 87015.23978874\n",
      "Iteration 179, loss = 87009.86308458\n",
      "Iteration 180, loss = 87004.50426054\n",
      "Iteration 181, loss = 86999.15976909\n",
      "Iteration 182, loss = 86993.80340764\n",
      "Iteration 183, loss = 86988.48886418\n",
      "Iteration 184, loss = 86983.18151893\n",
      "Iteration 185, loss = 86977.84734410\n",
      "Iteration 186, loss = 86972.59546134\n",
      "Iteration 187, loss = 86967.31780568\n",
      "Iteration 188, loss = 86962.05277870\n",
      "Iteration 189, loss = 86956.79622022\n",
      "Iteration 190, loss = 86951.57812732\n",
      "Iteration 191, loss = 86946.34404216\n",
      "Iteration 192, loss = 86941.12459582\n",
      "Iteration 193, loss = 86935.92395645\n",
      "Iteration 194, loss = 86930.73989275\n",
      "Iteration 195, loss = 86925.58600147\n",
      "Iteration 196, loss = 86920.40693483\n",
      "Iteration 197, loss = 86915.24223413\n",
      "Iteration 198, loss = 86910.11574489\n",
      "Iteration 199, loss = 86904.97789931\n",
      "Iteration 200, loss = 86899.87744766\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MLPRegressor(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(25, 10, 2, 10, 25), learning_rate='constant',\n",
       "             learning_rate_init=0.0001, max_fun=15000, max_iter=200,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "             tol=1e-07, validation_fraction=0.1, verbose=True,\n",
       "             warm_start=False)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Encoder structure\n",
    "n_encoder1 = 25\n",
    "n_encoder2 = 10\n",
    "\n",
    "n_latent = 2\n",
    "\n",
    "encoding_layers_count = 3\n",
    "\n",
    "# Decoder structure\n",
    "n_decoder2 = 10\n",
    "n_decoder1 = 25\n",
    "\n",
    "hidden_layer_sizes = (\n",
    "    n_encoder1, \n",
    "    n_encoder2, \n",
    "    n_latent, \n",
    "    n_decoder2, \n",
    "    n_decoder1\n",
    ")\n",
    "reset_random_seed()\n",
    "auto_encoder = MLPRegressor(\n",
    "                   hidden_layer_sizes=hidden_layer_sizes, \n",
    "                   activation = 'tanh', \n",
    "                   solver = 'adam', \n",
    "                   learning_rate_init = 0.0001, \n",
    "                   max_iter = 200, \n",
    "                   tol = 0.0000001, \n",
    "                   verbose = True\n",
    ")\n",
    "auto_encoder.fit(X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5536332179930796"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# soft max\n",
    "accuracy_score(y_test, 1 - np.argmax(encoder(X_test, auto_encoder), axis=1))"
   ]
  },
  {
   "source": [
    "## AUTO ENCODER + SVM + Standarad Scaler\n",
    "### Accuracy 0.9332350104940723"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9332350104940723"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "reset_random_seed()\n",
    "svm = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svm.fit(encoder(X, auto_encoder), y)\n",
    "accuracy_score(y_test, svm.predict(encoder(X_test, auto_encoder)))"
   ]
  },
  {
   "source": [
    "## AUTO ENCODER {Multi laten} + SVM + Standarad Scaler\n",
    "### Accuracy 0.9965965171025015"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 87919.67346394\n",
      "Iteration 2, loss = 87907.66099604\n",
      "Iteration 3, loss = 87895.80083362\n",
      "Iteration 4, loss = 87884.80337463\n",
      "Iteration 5, loss = 87875.00997284\n",
      "Iteration 6, loss = 87865.39050097\n",
      "Iteration 7, loss = 87855.57328140\n",
      "Iteration 8, loss = 87846.54585992\n",
      "Iteration 9, loss = 87838.14641115\n",
      "Iteration 10, loss = 87830.13033517\n",
      "Iteration 11, loss = 87822.23193690\n",
      "Iteration 12, loss = 87814.42633271\n",
      "Iteration 13, loss = 87806.52859203\n",
      "Iteration 14, loss = 87798.56408199\n",
      "Iteration 15, loss = 87790.52981879\n",
      "Iteration 16, loss = 87782.36156212\n",
      "Iteration 17, loss = 87774.08264285\n",
      "Iteration 18, loss = 87765.60897127\n",
      "Iteration 19, loss = 87756.99505153\n",
      "Iteration 20, loss = 87748.22758430\n",
      "Iteration 21, loss = 87739.21256325\n",
      "Iteration 22, loss = 87730.11401503\n",
      "Iteration 23, loss = 87720.75464798\n",
      "Iteration 24, loss = 87711.21995779\n",
      "Iteration 25, loss = 87701.45228452\n",
      "Iteration 26, loss = 87691.58698806\n",
      "Iteration 27, loss = 87681.39493524\n",
      "Iteration 28, loss = 87671.13760849\n",
      "Iteration 29, loss = 87660.61810724\n",
      "Iteration 30, loss = 87649.87948615\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9965965171025015"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "reset_random_seed()\n",
    "AE = MLPRegressor(\n",
    "    hidden_layer_sizes=(100,30,7,30,50), \n",
    "    activation = 'tanh', \n",
    "    solver = 'adam', \n",
    "    learning_rate_init = 0.0001, \n",
    "    max_iter = 30, \n",
    "    tol = 0.0000001, \n",
    "    verbose = True\n",
    ")\n",
    "AE.fit(X, X)\n",
    "svm = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svm.fit(encoder(X, AE), y)\n",
    "accuracy_score(y_test, svm.predict(encoder(X_test, AE)))"
   ]
  },
  {
   "source": [
    "## XGBOOST\n",
    "### Acuuracy 100%"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=1.0, loss='deviance', max_depth=1,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "reset_random_seed()\n",
    "xgboost = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "xgboost.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "accuracy_score(y_test, xgboost.predict(X_test))"
   ]
  },
  {
   "source": [
    "## Runtime\n",
    "\n",
    "> Real-time responses are often understood to be in the order of milliseconds, and sometimes microseconds. \n",
    " \n",
    "So xgboost can be concider aa real-time process"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "prediction_times = []\n",
    "\n",
    "for x in X_test:\n",
    "    x = x.reshape(1,-1)\n",
    "    t0 = time.time()\n",
    "    xgboost.predict(x)\n",
    "    t1 = time.time()\n",
    "    prediction_times.append(t1 - t0)\n",
    "\n",
    "prediction_times = np.array(prediction_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "prediction_times ~ N(0.0002532774568030078, 4.119685509044161e-05)\nprediction_times slowers=0.8068084716796875 ms (miliseconds)\nprediction_times fastest=0.2262592315673828 ms (miliseconds)\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction_times ~ N({np.mean(prediction_times)}, {np.std(prediction_times)})\")\n",
    "print(f\"prediction_times slowers={prediction_times.max()*1000} ms (miliseconds)\")\n",
    "print(f\"prediction_times fastest={prediction_times.min()*1000} ms (miliseconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "plarfomr machine x86_64\nplarfomr system Linux\nplarfomr processor x86_64\nplarfomr detail Linux-4.15.0-132-generic-x86_64-with-glibc2.10\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(f\"plarfomr machine {platform.machine()}\")\n",
    "print(f\"plarfomr system {platform.system()}\")\n",
    "print(f\"plarfomr processor {platform.processor()}\")\n",
    "print(f\"plarfomr detail {platform.platform()}\")"
   ]
  }
 ]
}