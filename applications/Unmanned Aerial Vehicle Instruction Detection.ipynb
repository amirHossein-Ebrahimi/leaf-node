{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "**The dataset** is collected from UCI Machine Learning Repository through the following [link](https://archive.ics.uci.edu/ml/datasets/Unmanned+Aerial+Vehicle+%28UAV%29+Intrusion+Detection)\n",
    "\n",
    "This application is working in first dataset (Bidirectional-flow/Parrot Bebop1), combined first dataset can be [downloaded](http://mason.gmu.edu/~lzhao9/materials/data/UAV/data/pub_dataset1.mat) from Liang Zhao homepage.Bidirectional-flow mode will involve 9 features × 2 sources × 3 direction flow = 54 features for more info visit this [link](http://mason.gmu.edu/~lzhao9/materials/data/UAV/)\n",
    "\n",
    "extract data with its default name `pub_dataset1.mat` in `__data__` directory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['D', 'H', 'data_te', 'data_tr'])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# use HDF reader for matlab v7.3 files\n",
    "data = { k:np.array(v).T for k, v in h5py.File('./__data__/pub_dataset1.mat').items()}\n",
    "data.keys()"
   ]
  },
  {
   "source": [
    "$n$ is the number of training samples   \n",
    "$k$ is the number of feature   \n",
    "$n^{\\prime}$ is the number of testing samples    \n",
    "$k^{\\prime}$ is the number of feature computational components and k is the numbe of features.  \n",
    "The last column of `data_te` and `data_tr` is the label: `1 means UAV, 0 otherwise`\n",
    "\n",
    "--- \n",
    "$\\text{data_tr} \\in R^{n×(k+1)}$   \n",
    "$\\text{data_te} \\in R^{n^{\\prime}×(k+1)}$   \n",
    "$D \\in R^{k×1}$. The generation runtime for each feature.  \n",
    "$H \\in R^{k^{\\prime}×k}$. The incident matrix of the feature computational hypergraph (see the paper for details). \n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seed(seed=1917):\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['data_tr'][:, :-1]\n",
    "y = data['data_tr'][:, -1]\n",
    "\n",
    "X_test = data['data_te'][:, :-1]\n",
    "y_test = data['data_te'][:, -1]"
   ]
  },
  {
   "source": [
    "## MLP\n",
    "### Accuracy 0.9937035566396278"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9937035566396278"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "reset_random_seed()\n",
    "model = MLPClassifier()\n",
    "model.fit(X, y)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(data, ae, encoding_layers_count=3):\n",
    "    data = np.asmatrix(data)\n",
    "\n",
    "    layer = data\n",
    "    for i in range(encoding_layers_count):\n",
    "        layer = layer*ae.coefs_[i] + ae.intercepts_[i]\n",
    "        encoder1 = np.tanh(layer)\n",
    "    \n",
    "    return np.asarray(layer)"
   ]
  },
  {
   "source": [
    "## Auto Encoder\n",
    "### Accuracy 0.5536332179930796"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 87931.33384487\n",
      "Iteration 2, loss = 87927.27633029\n",
      "Iteration 3, loss = 87923.67704696\n",
      "Iteration 4, loss = 87920.15169486\n",
      "Iteration 5, loss = 87916.61288665\n",
      "Iteration 6, loss = 87913.05216356\n",
      "Iteration 7, loss = 87909.50056686\n",
      "Iteration 8, loss = 87905.91427044\n",
      "Iteration 9, loss = 87902.25035234\n",
      "Iteration 10, loss = 87898.56404509\n",
      "Iteration 11, loss = 87894.86006997\n",
      "Iteration 12, loss = 87891.06019651\n",
      "Iteration 13, loss = 87887.24699085\n",
      "Iteration 14, loss = 87883.28777192\n",
      "Iteration 15, loss = 87879.31143292\n",
      "Iteration 16, loss = 87875.17031097\n",
      "Iteration 17, loss = 87870.93969102\n",
      "Iteration 18, loss = 87866.57590696\n",
      "Iteration 19, loss = 87862.08969428\n",
      "Iteration 20, loss = 87857.48735840\n",
      "Iteration 21, loss = 87852.74302202\n",
      "Iteration 22, loss = 87847.82784237\n",
      "Iteration 23, loss = 87842.75453955\n",
      "Iteration 24, loss = 87837.48416242\n",
      "Iteration 25, loss = 87832.05479651\n",
      "Iteration 26, loss = 87826.48792148\n",
      "Iteration 27, loss = 87820.72014286\n",
      "Iteration 28, loss = 87814.80765226\n",
      "Iteration 29, loss = 87808.73566969\n",
      "Iteration 30, loss = 87802.47585831\n",
      "Iteration 31, loss = 87796.07776747\n",
      "Iteration 32, loss = 87789.53148574\n",
      "Iteration 33, loss = 87782.83124715\n",
      "Iteration 34, loss = 87775.93150466\n",
      "Iteration 35, loss = 87768.93858612\n",
      "Iteration 36, loss = 87761.77471294\n",
      "Iteration 37, loss = 87754.48279848\n",
      "Iteration 38, loss = 87747.03350190\n",
      "Iteration 39, loss = 87739.45084625\n",
      "Iteration 40, loss = 87731.77493941\n",
      "Iteration 41, loss = 87723.92327233\n",
      "Iteration 42, loss = 87715.97480203\n",
      "Iteration 43, loss = 87707.90391089\n",
      "Iteration 44, loss = 87699.71767180\n",
      "Iteration 45, loss = 87691.43366700\n",
      "Iteration 46, loss = 87683.01862303\n",
      "Iteration 47, loss = 87674.54935320\n",
      "Iteration 48, loss = 87665.98176390\n",
      "Iteration 49, loss = 87657.30179911\n",
      "Iteration 50, loss = 87648.55585770\n",
      "Iteration 51, loss = 87639.79463632\n",
      "Iteration 52, loss = 87630.90749724\n",
      "Iteration 53, loss = 87621.89660176\n",
      "Iteration 54, loss = 87612.92862593\n",
      "Iteration 55, loss = 87603.85266562\n",
      "Iteration 56, loss = 87594.78050564\n",
      "Iteration 57, loss = 87585.61505008\n",
      "Iteration 58, loss = 87576.43628298\n",
      "Iteration 59, loss = 87567.25934729\n",
      "Iteration 60, loss = 87558.01403248\n",
      "Iteration 61, loss = 87548.76943169\n",
      "Iteration 62, loss = 87539.49833788\n",
      "Iteration 63, loss = 87530.24967638\n",
      "Iteration 64, loss = 87520.94726084\n",
      "Iteration 65, loss = 87511.70528402\n",
      "Iteration 66, loss = 87502.40144405\n",
      "Iteration 67, loss = 87493.12045515\n",
      "Iteration 68, loss = 87483.85936554\n",
      "Iteration 69, loss = 87474.61386050\n",
      "Iteration 70, loss = 87465.37895507\n",
      "Iteration 71, loss = 87456.15353986\n",
      "Iteration 72, loss = 87446.95871643\n",
      "Iteration 73, loss = 87437.77004891\n",
      "Iteration 74, loss = 87428.63790428\n",
      "Iteration 75, loss = 87419.54465338\n",
      "Iteration 76, loss = 87410.43034764\n",
      "Iteration 77, loss = 87401.41358020\n",
      "Iteration 78, loss = 87392.35865460\n",
      "Iteration 79, loss = 87383.34011257\n",
      "Iteration 80, loss = 87374.39519936\n",
      "Iteration 81, loss = 87365.50357382\n",
      "Iteration 82, loss = 87356.59030020\n",
      "Iteration 83, loss = 87347.69866841\n",
      "Iteration 84, loss = 87338.88580118\n",
      "Iteration 85, loss = 87330.06137232\n",
      "Iteration 86, loss = 87321.35872466\n",
      "Iteration 87, loss = 87312.58419563\n",
      "Iteration 88, loss = 87303.89128064\n",
      "Iteration 89, loss = 87295.25247120\n",
      "Iteration 90, loss = 87286.57945667\n",
      "Iteration 91, loss = 87277.98230777\n",
      "Iteration 92, loss = 87269.43803034\n",
      "Iteration 93, loss = 87260.91346321\n",
      "Iteration 94, loss = 87252.39419072\n",
      "Iteration 95, loss = 87243.93261522\n",
      "Iteration 96, loss = 87235.48095255\n",
      "Iteration 97, loss = 87227.12192179\n",
      "Iteration 98, loss = 87218.76237091\n",
      "Iteration 99, loss = 87210.40878548\n",
      "Iteration 100, loss = 87202.15199370\n",
      "Iteration 101, loss = 87193.87967531\n",
      "Iteration 102, loss = 87185.68153833\n",
      "Iteration 103, loss = 87177.50203574\n",
      "Iteration 104, loss = 87169.37266684\n",
      "Iteration 105, loss = 87161.26538304\n",
      "Iteration 106, loss = 87153.21450213\n",
      "Iteration 107, loss = 87145.17222603\n",
      "Iteration 108, loss = 87137.22443024\n",
      "Iteration 109, loss = 87129.27044987\n",
      "Iteration 110, loss = 87121.38639599\n",
      "Iteration 111, loss = 87113.51791019\n",
      "Iteration 112, loss = 87105.71083072\n",
      "Iteration 113, loss = 87097.91219105\n",
      "Iteration 114, loss = 87090.18422163\n",
      "Iteration 115, loss = 87082.49800440\n",
      "Iteration 116, loss = 87074.81498586\n",
      "Iteration 117, loss = 87067.21058831\n",
      "Iteration 118, loss = 87059.60206621\n",
      "Iteration 119, loss = 87052.05269003\n",
      "Iteration 120, loss = 87044.50526425\n",
      "Iteration 121, loss = 87037.03601600\n",
      "Iteration 122, loss = 87029.60919743\n",
      "Iteration 123, loss = 87022.18733633\n",
      "Iteration 124, loss = 87014.80586438\n",
      "Iteration 125, loss = 87007.40938273\n",
      "Iteration 126, loss = 87000.10851503\n",
      "Iteration 127, loss = 86992.84942102\n",
      "Iteration 128, loss = 86985.57098040\n",
      "Iteration 129, loss = 86978.33083371\n",
      "Iteration 130, loss = 86971.17547065\n",
      "Iteration 131, loss = 86963.99051417\n",
      "Iteration 132, loss = 86956.85204231\n",
      "Iteration 133, loss = 86949.74695619\n",
      "Iteration 134, loss = 86942.66161581\n",
      "Iteration 135, loss = 86935.61104318\n",
      "Iteration 136, loss = 86928.59706389\n",
      "Iteration 137, loss = 86921.58605047\n",
      "Iteration 138, loss = 86914.63899222\n",
      "Iteration 139, loss = 86907.69877008\n",
      "Iteration 140, loss = 86900.71930641\n",
      "Iteration 141, loss = 86893.83139136\n",
      "Iteration 142, loss = 86886.99236620\n",
      "Iteration 143, loss = 86880.14128770\n",
      "Iteration 144, loss = 86873.28851118\n",
      "Iteration 145, loss = 86866.46988048\n",
      "Iteration 146, loss = 86859.66346749\n",
      "Iteration 147, loss = 86852.91292288\n",
      "Iteration 148, loss = 86846.17924899\n",
      "Iteration 149, loss = 86839.44698108\n",
      "Iteration 150, loss = 86832.72441132\n",
      "Iteration 151, loss = 86826.00853816\n",
      "Iteration 152, loss = 86819.37780957\n",
      "Iteration 153, loss = 86812.71238917\n",
      "Iteration 154, loss = 86806.06493633\n",
      "Iteration 155, loss = 86799.43819079\n",
      "Iteration 156, loss = 86792.86501268\n",
      "Iteration 157, loss = 86786.29829227\n",
      "Iteration 158, loss = 86779.71565883\n",
      "Iteration 159, loss = 86773.15103172\n",
      "Iteration 160, loss = 86766.64240604\n",
      "Iteration 161, loss = 86760.13545250\n",
      "Iteration 162, loss = 86753.62260381\n",
      "Iteration 163, loss = 86747.15313940\n",
      "Iteration 164, loss = 86740.68805669\n",
      "Iteration 165, loss = 86734.23820026\n",
      "Iteration 166, loss = 86727.78321346\n",
      "Iteration 167, loss = 86721.37752965\n",
      "Iteration 168, loss = 86714.97694257\n",
      "Iteration 169, loss = 86708.58286027\n",
      "Iteration 170, loss = 86702.19113600\n",
      "Iteration 171, loss = 86695.81898234\n",
      "Iteration 172, loss = 86689.50456478\n",
      "Iteration 173, loss = 86683.10221301\n",
      "Iteration 174, loss = 86676.79845702\n",
      "Iteration 175, loss = 86670.49342056\n",
      "Iteration 176, loss = 86664.16970070\n",
      "Iteration 177, loss = 86657.87761274\n",
      "Iteration 178, loss = 86651.58200307\n",
      "Iteration 179, loss = 86645.32660267\n",
      "Iteration 180, loss = 86639.04979284\n",
      "Iteration 181, loss = 86632.81254759\n",
      "Iteration 182, loss = 86626.58497741\n",
      "Iteration 183, loss = 86620.34856695\n",
      "Iteration 184, loss = 86614.11200767\n",
      "Iteration 185, loss = 86607.92924596\n",
      "Iteration 186, loss = 86601.71223569\n",
      "Iteration 187, loss = 86595.54658308\n",
      "Iteration 188, loss = 86589.34483389\n",
      "Iteration 189, loss = 86583.17885879\n",
      "Iteration 190, loss = 86577.04398355\n",
      "Iteration 191, loss = 86570.89284145\n",
      "Iteration 192, loss = 86564.76791984\n",
      "Iteration 193, loss = 86558.62846774\n",
      "Iteration 194, loss = 86552.51391563\n",
      "Iteration 195, loss = 86546.41368662\n",
      "Iteration 196, loss = 86540.29661756\n",
      "Iteration 197, loss = 86534.22678476\n",
      "Iteration 198, loss = 86528.13739616\n",
      "Iteration 199, loss = 86522.03752103\n",
      "Iteration 200, loss = 86515.97958200\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MLPRegressor(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(34, 25, 2, 25, 34), learning_rate='constant',\n",
       "             learning_rate_init=0.0001, max_fun=15000, max_iter=200,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "             tol=1e-07, validation_fraction=0.1, verbose=True,\n",
       "             warm_start=False)"
      ]
     },
     "metadata": {},
     "execution_count": 187
    }
   ],
   "source": [
    "# Encoder structure\n",
    "n_encoder2 = 25\n",
    "n_encoder3 = 10\n",
    "\n",
    "n_latent = 2\n",
    "\n",
    "encoding_layers_count = 3\n",
    "\n",
    "# Decoder structure\n",
    "n_decoder3 = 10\n",
    "n_decoder2 = 25\n",
    "\n",
    "hidden_layer_sizes = (\n",
    "    n_encoder1, \n",
    "    n_encoder2, \n",
    "    n_latent, \n",
    "    n_decoder2, \n",
    "    n_decoder1\n",
    ")\n",
    "reset_random_seed()\n",
    "auto_encoder = MLPRegressor(\n",
    "                   hidden_layer_sizes=hidden_layer_sizes, \n",
    "                   activation = 'tanh', \n",
    "                   solver = 'adam', \n",
    "                   learning_rate_init = 0.0001, \n",
    "                   max_iter = 200, \n",
    "                   tol = 0.0000001, \n",
    "                   verbose = True\n",
    ")\n",
    "auto_encoder.fit(X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5536332179930796"
      ]
     },
     "metadata": {},
     "execution_count": 190
    }
   ],
   "source": [
    "# soft max\n",
    "accuracy_score(y_test, np.argmax(encoder(X_test, auto_encoder), axis=1))"
   ]
  },
  {
   "source": [
    "## AUTO ENCODER + SVM + Standarad Scaler\n",
    "### Accuracy 0.9157637982869136"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9157637982869136"
      ]
     },
     "metadata": {},
     "execution_count": 168
    }
   ],
   "source": [
    "svm = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svm.fit(encoder(X), y)\n",
    "accuracy_score(y_test, svm.predict(encoder(X_test)))"
   ]
  },
  {
   "source": [
    "## AUTO ENCODER {Multi laten} + SVM + Standarad Scaler\n",
    "### Accuracy 0.9934766577797947"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 87908.89191948\n",
      "Iteration 2, loss = 87900.83005277\n",
      "Iteration 3, loss = 87892.89103523\n",
      "Iteration 4, loss = 87884.88454356\n",
      "Iteration 5, loss = 87876.84062800\n",
      "Iteration 6, loss = 87868.60495798\n",
      "Iteration 7, loss = 87860.07827601\n",
      "Iteration 8, loss = 87851.35748410\n",
      "Iteration 9, loss = 87842.48923491\n",
      "Iteration 10, loss = 87833.32172414\n",
      "Iteration 11, loss = 87823.89700553\n",
      "Iteration 12, loss = 87814.26890947\n",
      "Iteration 13, loss = 87804.33933314\n",
      "Iteration 14, loss = 87794.17880984\n",
      "Iteration 15, loss = 87783.76944986\n",
      "Iteration 16, loss = 87773.08604304\n",
      "Iteration 17, loss = 87762.15706269\n",
      "Iteration 18, loss = 87751.00045754\n",
      "Iteration 19, loss = 87739.61474835\n",
      "Iteration 20, loss = 87728.02415342\n",
      "Iteration 21, loss = 87716.09393861\n",
      "Iteration 22, loss = 87703.94774254\n",
      "Iteration 23, loss = 87691.54380877\n",
      "Iteration 24, loss = 87678.88100280\n",
      "Iteration 25, loss = 87666.02841067\n",
      "Iteration 26, loss = 87653.07801898\n",
      "Iteration 27, loss = 87639.95885990\n",
      "Iteration 28, loss = 87626.52333795\n",
      "Iteration 29, loss = 87613.13455951\n",
      "Iteration 30, loss = 87599.51710905\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9934766577797947"
      ]
     },
     "metadata": {},
     "execution_count": 173
    }
   ],
   "source": [
    "AE = MLPRegressor(\n",
    "    hidden_layer_sizes=(100,30,7,30,50), \n",
    "    activation = 'tanh', \n",
    "    solver = 'adam', \n",
    "    learning_rate_init = 0.0001, \n",
    "    max_iter = 30, \n",
    "    tol = 0.0000001, \n",
    "    verbose = True\n",
    ")\n",
    "AE.fit(X, X)\n",
    "svm = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svm.fit(encoder(X, 3, AE), y)\n",
    "accuracy_score(y_test, svm.predict(encoder(X_test, 3, AE)))"
   ]
  },
  {
   "source": [
    "proposed method will be the follwoing  \n",
    "- classify   \n",
    "- print wrong data   \n",
    "- train model on wrong data such as svm  \n",
    "- use svm for that classified datas\n",
    "- o.w use mlp normal model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## XGBOOST\n",
    "### Acuuracy 100%"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=1.0, loss='deviance', max_depth=1,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "metadata": {},
     "execution_count": 184
    }
   ],
   "source": [
    "random\n",
    "xgboost = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "xgboost.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 185
    }
   ],
   "source": [
    "accuracy_score(y_test, xgboost.predict(X_test))"
   ]
  },
  {
   "source": [
    "## Runtime\n",
    "\n",
    "> Real-time responses are often understood to be in the order of milliseconds, and sometimes microseconds. \n",
    " \n",
    "So xgboost can be concider aa real-time process"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "prediction_times = []\n",
    "\n",
    "for x in X_test:\n",
    "    x = x.reshape(1,-1)\n",
    "    t0 = time.time()\n",
    "    xgboost.predict(x)\n",
    "    t1 = time.time()\n",
    "    prediction_times.append(t1 - t0)\n",
    "\n",
    "prediction_times = np.array(prediction_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "prediction_times ~ N(0.0002549129820186462, 3.486487932831444e-05)\nprediction_times slowers=0.9953975677490234 ms (miliseconds)\nprediction_times fastest=0.2238750457763672 ms (miliseconds)\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction_times ~ N({np.mean(prediction_times)}, {np.std(prediction_times)})\")\n",
    "print(f\"prediction_times slowers={prediction_times.max()*1000} ms (miliseconds)\")\n",
    "print(f\"prediction_times fastest={prediction_times.min()*1000} ms (miliseconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "plarfomr machine x86_64\n",
      "plarfomr system Linux\n",
      "plarfomr processor x86_64\n",
      "plarfomr detail Linux-4.15.0-132-generic-x86_64-with-glibc2.10\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(f\"plarfomr machine {platform.machine()}\")\n",
    "print(f\"plarfomr system {platform.system()}\")\n",
    "print(f\"plarfomr processor {platform.processor()}\")\n",
    "print(f\"plarfomr detail {platform.platform()}\")"
   ]
  }
 ]
}