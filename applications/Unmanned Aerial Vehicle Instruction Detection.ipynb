{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "**The dataset** is collected from UCI Machine Learning Repository through the following [link](https://archive.ics.uci.edu/ml/datasets/Unmanned+Aerial+Vehicle+%28UAV%29+Intrusion+Detection)\n",
    "\n",
    "This application is working in first dataset (Bidirectional-flow/Parrot Bebop1), combined first dataset can be [downloaded](http://mason.gmu.edu/~lzhao9/materials/data/UAV/data/pub_dataset1.mat) from Liang Zhao homepage.Bidirectional-flow mode will involve 9 features × 2 sources × 3 direction flow = 54 features for more info visit this [link](http://mason.gmu.edu/~lzhao9/materials/data/UAV/)\n",
    "\n",
    "extract data with its default name `pub_dataset1.mat` in `__data__` directory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['D', 'H', 'data_te', 'data_tr'])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# use HDF reader for matlab v7.3 files\n",
    "data = { k:np.array(v).T for k, v in h5py.File('./__data__/pub_dataset1.mat').items()}\n",
    "data.keys()"
   ]
  },
  {
   "source": [
    "$n$ is the number of training samples   \n",
    "$k$ is the number of feature   \n",
    "$n^{\\prime}$ is the number of testing samples    \n",
    "$k^{\\prime}$ is the number of feature computational components and k is the numbe of features.  \n",
    "The last column of `data_te` and `data_tr` is the label: `1 means UAV, 0 otherwise`\n",
    "\n",
    "--- \n",
    "$\\text{data_tr} \\in R^{n×(k+1)}$   \n",
    "$\\text{data_te} \\in R^{n^{\\prime}×(k+1)}$   \n",
    "$D \\in R^{k×1}$. The generation runtime for each feature.  \n",
    "$H \\in R^{k^{\\prime}×k}$. The incident matrix of the feature computational hypergraph (see the paper for details). \n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seed(seed=1917):\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['data_tr'][:, :-1]\n",
    "y = data['data_tr'][:, -1]\n",
    "\n",
    "X_test = data['data_te'][:, :-1]\n",
    "y_test = data['data_te'][:, -1]"
   ]
  },
  {
   "source": [
    "## MLP\n",
    "### Accuracy 0.9937035566396278"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9937035566396278"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "reset_random_seed()\n",
    "model = MLPClassifier()\n",
    "model.fit(X, y)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(data, ae, encoding_layers_count=3):\n",
    "    data = np.asmatrix(data)\n",
    "\n",
    "    layer = data\n",
    "    for i in range(encoding_layers_count):\n",
    "        layer = layer*ae.coefs_[i] + ae.intercepts_[i]\n",
    "        encoder1 = np.tanh(layer)\n",
    "    \n",
    "    return np.asarray(layer)"
   ]
  },
  {
   "source": [
    "## Auto Encoder\n",
    "### Accuracy 0.5536332179930796"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'n_enco' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-75bff5b13185>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m hidden_layer_sizes = (\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mn_enco\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mder1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mn_encoder2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mn_latent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_enco' is not defined"
     ]
    }
   ],
   "source": [
    "# Encoder structure\n",
    "n_encoder1 = 25\n",
    "n_encoder2 = 10\n",
    "\n",
    "n_latent = 2\n",
    "\n",
    "encoding_layers_count = 3\n",
    "\n",
    "# Decoder structure\n",
    "n_decoder2 = 10\n",
    "n_decoder1 = 25\n",
    "\n",
    "hidden_layer_sizes = (\n",
    "    n_encoder1, \n",
    "    n_encoder2, \n",
    "    n_latent, \n",
    "    n_decoder2, \n",
    "    n_decoder1\n",
    ")\n",
    "reset_random_seed()\n",
    "auto_encoder = MLPRegressor(\n",
    "                   hidden_layer_sizes=hidden_layer_sizes, \n",
    "                   activation = 'tanh', \n",
    "                   solver = 'adam', \n",
    "                   learning_rate_init = 0.0001, \n",
    "                   max_iter = 200, \n",
    "                   tol = 0.0000001, \n",
    "                   verbose = True\n",
    ")\n",
    "auto_encoder.fit(X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft max\n",
    "accuracy_score(y_test, np.argmax(encoder(X_test, auto_encoder), axis=1))"
   ]
  },
  {
   "source": [
    "## AUTO ENCODER + SVM + Standarad Scaler\n",
    "### Accuracy 0.9157637982869136"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svm.fit(encoder(X, auto_encoder), y)\n",
    "accuracy_score(y_test, svm.predict(encoder(X_test, auto_encoder)))"
   ]
  },
  {
   "source": [
    "## AUTO ENCODER {Multi laten} + SVM + Standarad Scaler\n",
    "### Accuracy 0.9934766577797947"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = MLPRegressor(\n",
    "    hidden_layer_sizes=(100,30,7,30,50), \n",
    "    activation = 'tanh', \n",
    "    solver = 'adam', \n",
    "    learning_rate_init = 0.0001, \n",
    "    max_iter = 30, \n",
    "    tol = 0.0000001, \n",
    "    verbose = True\n",
    ")\n",
    "AE.fit(X, X)\n",
    "svm = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svm.fit(encoder(X, AE), y)\n",
    "accuracy_score(y_test, svm.predict(encoder(X_test, AE)))"
   ]
  },
  {
   "source": [
    "proposed method will be the follwoing  \n",
    "- classify   \n",
    "- print wrong data   \n",
    "- train model on wrong data such as svm  \n",
    "- use svm for that classified datas\n",
    "- o.w use mlp normal model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## XGBOOST\n",
    "### Acuuracy 100%"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random\n",
    "xgboost = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "xgboost.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, xgboost.predict(X_test))"
   ]
  },
  {
   "source": [
    "## Runtime\n",
    "\n",
    "> Real-time responses are often understood to be in the order of milliseconds, and sometimes microseconds. \n",
    " \n",
    "So xgboost can be concider aa real-time process"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "prediction_times = []\n",
    "\n",
    "for x in X_test:\n",
    "    x = x.reshape(1,-1)\n",
    "    t0 = time.time()\n",
    "    xgboost.predict(x)\n",
    "    t1 = time.time()\n",
    "    prediction_times.append(t1 - t0)\n",
    "\n",
    "prediction_times = np.array(prediction_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"prediction_times ~ N({np.mean(prediction_times)}, {np.std(prediction_times)})\")\n",
    "print(f\"prediction_times slowers={prediction_times.max()*1000} ms (miliseconds)\")\n",
    "print(f\"prediction_times fastest={prediction_times.min()*1000} ms (miliseconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "print(f\"plarfomr machine {platform.machine()}\")\n",
    "print(f\"plarfomr system {platform.system()}\")\n",
    "print(f\"plarfomr processor {platform.processor()}\")\n",
    "print(f\"plarfomr detail {platform.platform()}\")"
   ]
  }
 ]
}